---
layout: post
title: "머신러닝 기본개념"
author: "Oizys18"
categories: AI_ML
comments: true
tags: [AI, MachineLearning, Basic]
---

## 목차
* TOC
{:toc}
* * *


## 과대적합 Overfitting

- 모델이 엄청 유연해서 학습(훈련) 데이터는 잘 분류하지만, 실제 데이터에선 성능을 발휘하지 못하는 경우
- 일반성이 떨어진다는 뜻
- 너무 복잡한 패턴을 학습하는 경우, 잡음까지 학습하는 경우 등등 발생한다.

- 해결방법
  - 훈련 데이터를 더 많이 모은다. (훈련 데이터의 크기를 늘린다.)
  - 정규화 Regularization (규제,드롭아웃 등)를 통해 적당한 복잡도를 가지는 모델을 찾는다. 
  - 훈련 데이터의 잡음을 줄인다. (오류 수정, 이상치 제거) 

## 과소적합 Underfitting

- 과대적합의 반대, 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때 발생
- 해결방법
  - 파라미터가 더 많은 복잡한 모델을 선택
  - 모델의 제약을 줄이기 (규제 하이퍼파라미터 값 줄이기)
  - 조기종료 시점 (overfitting이 되기 전의 시점)까지 충분히 학습

## 일반화 Generalization

- 과대적합, 과소적합을 피하고 테스트 데이터에 대한 높은 성능을 갖추는 것

## 파라미터 Parameter (매개변수)

- ```
  A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.
  
  - They are required by the model when making predictions.
  - They values define the skill of the model on your problem.
  - They are estimated or learned from data.
  - They are often not set manually by the practitioner.
  - They are often saved as part of the learned model.
  ```

- 모델 내부에서 결정되는 변수, 데이터로부터 결정되는 값. 사용자에 의해 조정되지 않는다.

- ex) 선형 회귀의 계수,  정규분포의 평균과 표준편차 등 

## 하이퍼 파라미터 Hyperparameter

- ```
  A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.
  
  - They are often used in processes to help estimate model parameters.
  - They are often specified by the practitioner.
  - They can often be set using heuristics.
  - They are often tuned for a given predictive modeling problem.
  ```

- 모델링할 때 사용자가 직접 세팅해주는 값.

- 정해진 최적의 값이 없다. 즉, 휴리스틱과 경험법칙에 의해 결정되는 경우가 많다. 

- ex) learning rate, SVM의 C, sigma, KNN의 K 등

## 스트라이드 stride

- 입력층의 윈도우를 움직이면서 필터를 적용함으로써 은닉층의 뉴런을 형성할 때, 윈도우(필터)의 이동량을 의미한다.
  - 주로 1로 설정하지만 변경할 수 있다.

## ReLU 함수

- 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수

## 시그모이드 함수 sigmoid fuction

- 계단함수에 비해 완만한 곡선 형태로 비선형이다. 특정 경계를 기준으로 출력이 확 바뀌어버리는 계단함수와는 달리 시그모이드 함수는 완만하게 매끄럽게 변화하는데 이 매끄러움이 신경망 학습에서 중요하며 활성화 함수로 시그모이드 함수를 사용하는 이유이기도 하다.

## Epoch 
```
- One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE
- 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태
```
- 신경망에서 사용되는 역전파 알고리즘 중, 전체 데이터셋에 대하여 순방향 패스와 역방향 패스가 완료되면 한 번의 epoch가 진행되었다고 한다.
- 즉 전체 데이터 셋에 대해 한 번의 학습과정이 완료되었다는 뜻이다. 
- 적절한 epoch 값을 설정해야 underfitting(epoch 값이 너무 작음)과 overfitting(epoch값이 너무 큼)을 방지할 수 있다.

## batch size
- Total number of training examples present in a single batch.
- 한 번의 batch마다 주는 데이터 샘플의 size. 

## iteration
- The number of passes to complete one epoch.
- epoch를 나누어서 실행하는 횟수
```
메모리의 한계와 속도 저하 때문에 대부분의 경우에는 한 번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 그래서 데이터를 나누어서 주게 되는데 이때 몇 번 나누어서 주는가를 iteration, 각 iteration마다 주는 데이터 사이즈를 batch size라고 합니다.
```

## 경사하강법 Gradient Descent

- 오차를 역전파하여 계속 업데이트 하는 이유는 신경망을 통해 더 나은 결과 값을 내기 위해서 weight를 조정하는데 오차가 영향을 주기 때문이다. 이 때 좀 더 효율적으로 오차를 계산하기 위한 방법이다.

- 경사하강법은 너무나 많은 신경망 안의 가중치 조합을 모두 계산하면 시간이 오래걸리기 때문에 효율적으로 이를 하기위해 고안된 방법입이다. 

- 정확한 답을 얻지는 못할 수도 있다. 단계적으로 접근하는 것이기 때문에 만족스러운 정확도에 이를 때까지 계속해서 답을 찾아나가는 방식이다.

- 신경망의 오차를 경사하강법으로 최저 오차를 찾아나가는 방식이며 신경망의 계산 속도를 빠르게 한다.

- 낮은 쪽의 방향을 찾기 위해 오차함수를 현재 위치에서 미분한다.

- 함수의 최저점을 구하기 좋은 방법으로 신경망과 같이 계산해야 하는 양이 많고(선형대수학) 접근하기가 복잡한 수식에서 잘 작동

- 데이터가 불완전해도 유도리 있게 작동

## 정규화
### normalization

### standardization


# 참고

- https://sacko.tistory.com/